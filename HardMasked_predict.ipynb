{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HardMasked_FINALE_PREDICT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjNgbT0QLSOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7035a1fb-6b4a-4b2c-f5e4-2d72bb663d66"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install seqeval\r\n",
        "!pip install sentencepiece\r\n",
        "import sentencepiece as spm\r\n",
        "\r\n",
        "from seqeval.metrics import precision_score as seq_precision, recall_score as seq_recall, f1_score as seq_f1\r\n",
        "from transformers import AutoTokenizer, XLMRobertaModel, XLMRobertaForMaskedLM\r\n",
        "import json\r\n",
        "import logging\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import copy\r\n",
        "import os\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import torch.nn as nn \r\n",
        "from torch.nn import functional as F\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from easydict import EasyDict\r\n",
        "import gc\r\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\r\n",
        "from torch.optim import Adam\r\n",
        "import pickle\r\n",
        "import re\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 28.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=00d1b45291b13760cb0a75a6b5944f9e95612ab825150d9afd8716e483c5f40b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=0aff7581ec54c23cfacd6c34021e59396859164935d4e834b4ed3c5ac76385a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23qZr9TjLZJ7"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCIg9rRCLdv0"
      },
      "source": [
        "\r\n",
        "def num_parameters(parameters):\r\n",
        "    num = 0\r\n",
        "    for i in parameters:\r\n",
        "        num += len(i)\r\n",
        "    return num\r\n",
        "class Detector(nn.Module):\r\n",
        "    def __init__(self, input_dim,output_dim,  embedding_dim, num_layers, hidden_size):\r\n",
        "\r\n",
        "        super(Detector, self).__init__()\r\n",
        "        self.input_dim = input_dim\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.embedding_dim  = embedding_dim\r\n",
        "        self.num_layers = num_layers\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.embedding = nn.Embedding(num_embeddings = self.input_dim, embedding_dim = self.embedding_dim, )\r\n",
        "        self.LSTM = nn.LSTM(input_size = self.embedding_dim, hidden_size= self.hidden_size, num_layers = self.num_layers, \r\n",
        "                            batch_first = True, dropout = 0.1, bidirectional = True)\r\n",
        "        self.linear = nn.Linear(self.hidden_size*2, self.output_dim)\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "    def forward(self, x):\r\n",
        "        emb = self.embedding(x)\r\n",
        "        outputs, (h_n, h_c) = self.LSTM(emb)\r\n",
        "        logits = self.linear(outputs)\r\n",
        "\r\n",
        "        p = self.sigmoid(logits)\r\n",
        "        return p\r\n",
        "\r\n",
        "\r\n",
        "class HardMasked(nn.Module):\r\n",
        "    def __init__(self, detector, MaskedLM, detector_tokenizer, maskedlm_tokenzier,device ):\r\n",
        "        super(HardMasked, self).__init__()\r\n",
        "\r\n",
        "        self.detector = detector.to(device)\r\n",
        "        self.MaskedLM = MaskedLM.to(device)\r\n",
        "        self.detector_tokenizer = detector_tokenizer\r\n",
        "        self.maskedlm_tokenizer = maskedlm_tokenizer\r\n",
        "        self.use_device = device\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, s):\r\n",
        "        maskedlm_features = self.prepare_input(s)\r\n",
        "        outputs = MaskedLM(input_ids = torch.tensor([maskedlm_features['input_ids']], dtype = torch.long, device = self.use_device), \r\n",
        "                            attention_mask = torch.tensor([maskedlm_features['attention_mask']], dtype = torch.long, device = self.use_device) )\r\n",
        "        logits = outputs['logits'][0]\r\n",
        "        output_ids = torch.argmax(logits, dim = -1)\r\n",
        "        final_output = maskedlm_tokenizer.decode(output_ids)\r\n",
        "        return final_output\r\n",
        "\r\n",
        "\r\n",
        "    def prepare_input(self, s):\r\n",
        "\r\n",
        "        detector_input_ids = self.detector_tokenizer.encode(s, out_type = int)\r\n",
        "        detector_input_pieces = self.detector_tokenizer.id_to_piece(detector_input_ids)\r\n",
        "        detector_outputs = (self.detector(torch.tensor([detector_input_ids], dtype = torch.long, device = self.use_device))[0].reshape(1,-1) > 0.5).int()[0] \r\n",
        "\r\n",
        "        for i in range(1, len(detector_input_pieces)):\r\n",
        "            if detector_outputs[i] == 1:\r\n",
        "                detector_input_pieces[i] = ' <mask>'\r\n",
        "\r\n",
        "        masked_s = self.detector_tokenizer.decode(detector_input_pieces)\r\n",
        "        for i in range(5):\r\n",
        "            masked_s = re.sub(r'<mask>\\s<mask>', '<mask>', masked_s)\r\n",
        "\r\n",
        "        maskedlm_features = maskedlm_tokenizer(masked_s)\r\n",
        "\r\n",
        "        return maskedlm_features\r\n",
        "\r\n",
        "        \r\n",
        "            \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rBeJYKk3Mnt"
      },
      "source": [
        "# Load detector and XLM-R masked language model to create Hard-Masked XLM-R\r\n",
        "detector_path = '/content/drive/MyDrive/nlp_projects/Text_correction/all_data/Detector.pkl'\r\n",
        "\r\n",
        "MaskedLM = XLMRobertaForMaskedLM.from_pretrained('xlm-roberta-base')\r\n",
        "\r\n",
        "maskedlm_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\r\n",
        "\r\n",
        "detector_tokenizer_path = '/content/drive/MyDrive/nlp_projects/Text_correction/spm_tokenizer.model'\r\n",
        "\r\n",
        "detector_tokenizer = spm.SentencePieceProcessor(detector_tokenizer_path, )\r\n",
        "\r\n",
        "detector = torch.load(detector_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W3iyChU3tv0"
      },
      "source": [
        "model = HardMasked(detector, MaskedLM, detector_tokenizer, maskedlm_tokenizer, 'cuda')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2DOkTcoL8W7"
      },
      "source": [
        "s = 'Tôi vẫn luôn iu cô ấy với hết tấm lòng của mk'"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Kfd-WsExIkOC",
        "outputId": "8a491731-e75f-4936-c2aa-c12fee2fbe4e"
      },
      "source": [
        "model(s)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<s> Tôi vẫn luôn yêu cô ấy với hết tấm lòng của mình</s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj8O59DHZjot"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}